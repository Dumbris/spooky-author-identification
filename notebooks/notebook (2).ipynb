{
  "cells": [
    {
      "metadata": {
        "_uuid": "432bc27ee4c8d6c08f15d5bd0d230f2031468e9a",
        "_cell_guid": "307a5e01-3796-47f8-b9a5-8471e5035aef"
      },
      "cell_type": "markdown",
      "source": "http://www.learnnc.org/lp/editions/few/684\n\n*A writer’s style* is what sets his or her writing apart and makes it unique. Style is the way writing is dressed up (or down) to fit the specific context, purpose, or audience. Word choice, sentence fluency, and the writer’s voice — all contribute to the style of a piece of writing.\n\n**Elements of style**\n\nMany elements of writing contribute to an author’s style, but three of the most important are ***word choice***, sentence fluency, and voice.\n\nGood writers are concise and precise, weeding out unnecessary words and choosing the exact word to convey meaning. Precise words — active verbs, concrete nouns, specific adjectives — help the reader visualize the sentence. **Good writers use adjectives sparingly and adverbs rarely, letting their nouns and verbs do the work.**\n\nGood writers also choose words that contribute to the flow of a sentence. Polysyllabic words, alliteration, and consonance can be used to create sentences that roll off the tongue. Onomatopoeia and short, staccato words can be used to break up the rhythm of a sentence. "
    },
    {
      "metadata": {
        "_uuid": "3470d6a7e7deef262e4d54bd781c8aa86ed176b1",
        "_cell_guid": "4ce8eace-546a-49a6-aa26-31700d0c7e9a"
      },
      "cell_type": "markdown",
      "source": "My idea is to extract meaningful words - adjectives, nouns and verbs- that describe each writer style. "
    },
    {
      "metadata": {
        "_uuid": "858ed1f8824977c150a88d898b27c291027931ff",
        "_cell_guid": "eb437c77-c608-4e5b-9082-9e822f8df6e7",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regular expressions\nfrom collections import Counter # for counting\n# spacy\nimport spacy\nfrom spacy.parts_of_speech import ADV,ADJ,VERB\nfrom spacy.symbols import nsubj,ORTH,LEMMA, POS,PERSON\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "591a9c1841957314584e9410625a95889567a65a",
        "_cell_guid": "ba3fd402-ae77-447b-b46e-16ddfe148a09",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/train.csv\", encoding = \"utf8\")\n#train.shape\n#train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f6db60c74bcdbf92f348391669b3f3a61b6eba2f",
        "_cell_guid": "07a2a5b7-94d5-4d51-815a-310e47962230"
      },
      "cell_type": "markdown",
      "source": "For understanding spacy I'll focus on the first author EAP:"
    },
    {
      "metadata": {
        "_uuid": "bcea2f35bfa7a05b610ae791f0e097521b39a3fd",
        "_cell_guid": "7515d7a2-3182-4069-98c6-db9cd65d6a49",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "list_EAP = list(train.text[train.author == \"EAP\"])\nlist_EAP[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c393cc15dc0869787cf2bb5ab2f32812ff67ab81",
        "_cell_guid": "73156f2f-8080-4dd3-93fa-301830f5584f",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(list_EAP)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "73f671f1faa9ea090170917f0053a17a020eb953",
        "_cell_guid": "36af7dba-e837-40a5-93f8-36d33eb76175",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def list_to_text(l):\n    document = \" \"\n    for s in range(len(l)):\n        sent = \"\".join(l[s]).strip()\n        document += sent\n    return document",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a2e1ac933461599e41fce6a1e0b1e915b64cd1f",
        "_cell_guid": "19fa6151-9b6d-47e7-a108-5d661749a17c",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "text_EAP = list_to_text(list_EAP)\n#text_EAP\n#len(text_EAP)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "75289cc5de2c3bc8bd408321e2decc071b20b28b",
        "_cell_guid": "9d74345e-e5bf-4dd6-a034-cf6b1e47e288",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "nlp = spacy.load('en')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "556481e34525ccc5c4898364a8dc434f91a5dc21",
        "_cell_guid": "9faea3cc-d2e8-49cf-bb80-6da55b3b584e"
      },
      "cell_type": "markdown",
      "source": "https://spacy.io/usage/processing-pipelines \n\nWhen you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component."
    },
    {
      "metadata": {
        "_uuid": "2b2d3bea1da7f6301e50e2fb0f5f4b7b23831cb0",
        "_cell_guid": "e90c0ca7-c83f-4c2e-9655-ffbf298f505b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "eap = nlp(text_EAP)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1eb0a3ae488899f188a8ef03e918a93e1adc2028",
        "_cell_guid": "faa62180-e56e-4153-bd13-e85c14e2013a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#dir(eap)\n#eap.is_parsed\n#eap.is_tagged\n#eap.doc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa6eb1bc6230a2d6d4355e01c226dffc4427f4bd",
        "_cell_guid": "e8751db9-21b2-4a81-ab26-dd65702915d1",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Just to know how many sentences.\n#It's not importnat to focus on sentnces but words.\nsentences = [sentence.orth_ for sentence in eap.sents]\nprint(\"There are {} sentences.\".format(len(sentences)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7182e73fdb4432e0de01b067d1a0c940c126692d",
        "_cell_guid": "3a05a87b-80cd-4f04-8256-c1fe3c9af2af"
      },
      "cell_type": "markdown",
      "source": "**Noun phrases:**\n* **Text**: The original noun chunk text.\n* **Root text**: The original text of the word connecting the noun chunk to the rest of the parse.\n* **Root dep**: Dependcy relation connecting the root to its head.\n* **Root head text**: The text of the root token's head."
    },
    {
      "metadata": {
        "_uuid": "6c4769cb209c3fe94e9f5ffe9bf9df8037271c9c",
        "_cell_guid": "0ef5104b-ce12-4021-ad41-6ef4896cea69",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "chunks = [(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text) for chunk in eap.noun_chunks]\ndf = pd.DataFrame(chunks)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6991928a2190af4ddc6aa0eaa8c9dd19eea967d8",
        "_cell_guid": "d8b78381-5735-4a08-a832-c311aa2cf11a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "tokens = [(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children]) for token in eap]\ndf = pd.DataFrame(tokens)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f42a40171e3769fda5f1d3b1ed41e231187b637",
        "_cell_guid": "2e1d36da-77ea-487b-8d80-5e5d5a247a11"
      },
      "cell_type": "markdown",
      "source": "**Tokens:**\n* **Text**: The original token text.\n* **Dep**: The syntactic relation connecting child to head.\n* **Head text**: The original text of the token head.\n* **Head POS**: The part-of-speech tag of the token head.\n* **Children**: The immediate syntactic dependents of the token."
    },
    {
      "metadata": {
        "_uuid": "a5451ef9cd4c16e42433302476693e9b743d2ddc",
        "_cell_guid": "b8da17b7-b63a-443e-bd83-8a200fa3d3f4",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#AN example to check if \"'s\" and \"'nt\" are tokens with the spacy default tokenizer\ndoc = nlp(\"some cleaning does'nt hurt. so let's do it\")#some times mistake like this does'nt occur ==> more work to check spelling\ndoc1 = nlp(\"some cleaning doesn't hurt. so let's do it\")# correct this our example\ntokens = [token.orth_ for token in doc]\ntokens1 = [token.orth_ for token in doc1]\nprint(tokens,\"\\n\",tokens1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "825c711fc6e350cd52dcd63c780f3827c2966b95",
        "_cell_guid": "f0b3820f-1169-4beb-95e3-c5ac7bef279d"
      },
      "cell_type": "markdown",
      "source": "With spacy the default tokenizer is good. But may be we'll need to customize it. We will see in further steps."
    },
    {
      "metadata": {
        "_uuid": "cd4cf389c121cdd0e6989011f170278983878981",
        "_cell_guid": "fb533828-cbb3-4682-83cc-392aba5788cf"
      },
      "cell_type": "markdown",
      "source": "**Some counting:**"
    },
    {
      "metadata": {
        "_uuid": "1d454202321eb19fa2a7ec2f39ae13f8ef8af13d",
        "_cell_guid": "f0216fb3-9c72-443e-bc65-ac18429ac03d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "tokens = [token.orth_ for token in eap]\nprint(\"There are {} tokens\".format(len(tokens)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d098f2f7196ade9e43b675613b27bdfad6bdd90",
        "_cell_guid": "0ede50fb-419a-4e0c-8ef0-2b786ea31789",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Counter(tokens).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ecac99c72a8983268efedba641a845a66894c34",
        "_cell_guid": "c931b041-8942-4063-a195-9d71b8d7ec77",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#let's clean our eap doc\ncleaned_tokens = [token.orth_ for token in eap if not token.is_punct | token.is_space]\nprint(\"There are {} cleaned tokens\".format(len(cleaned_tokens)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1cd8e820d11da47534eacd148536874024719223",
        "_cell_guid": "3e18bd69-34c5-4118-bcd0-b17ed8045af5",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Counter(cleaned_tokens).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7f3bcf83a0832ed999ff95963f808f09649bdc00",
        "_cell_guid": "27e289bb-d75c-4682-9f0f-5a18b04546c8"
      },
      "cell_type": "markdown",
      "source": "(',', 17594) and ('.', 6486) are ignored after cleaning."
    },
    {
      "metadata": {
        "_uuid": "a7568cdebedbc54618711bc1890fce63a33ce5c5",
        "_cell_guid": "a9a1d96b-6e7f-46df-b72d-3f35c1c1a8d1"
      },
      "cell_type": "markdown",
      "source": "Lemmatization:\nLemmatisation is the process of reducing a word to its base form. Different uses of a word often have the same root meaning. For example, program, programmed and programming all essentially refer to the same thing. \nLemmatization avoids word duplication and, therefore, allows for the model to build a clearer picture of patterns of word usage across multiple documents.\n\n"
    },
    {
      "metadata": {
        "_uuid": "5b6d0bc516112a47ccd055acdd599d8c86572160",
        "_cell_guid": "b0fef597-1380-4ffd-9526-cc724a3debb4",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "lemmas = [(i,i.lemma_) for i in eap]\nprint(\"There are {} lemmas\".format(len(lemmas)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0310dc5bc06526a2f859a628e13f20b2a5d6cec7",
        "_cell_guid": "bf68630b-535a-493e-b47a-b3b63b59a2b8",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Counter(lemmas).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2123d1bcf3cb91fb8156d2d9f3261b434d9998ed",
        "_cell_guid": "fe3d15c5-c407-42ab-a4ed-8988d36f8644",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "POS Tagging\nPart-of-speech tagging is the process of assigning grammatical properties (e.g. noun, verb, adverb, adjective etc.) to words. Words that share the same POS tag tend to follow a similar syntactic structure and are useful in rule-based processes."
    },
    {
      "metadata": {
        "_uuid": "9d4c6d075c80dce407161c01c43443b063c3ebbf",
        "_cell_guid": "90362a48-5b94-4c16-a014-ca471eb9dee3",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "pos_tags = [(i, i.tag_) for i in eap]\nprint(\"There are {} pos_tags\".format(len(pos_tags)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c5f77e7fea1da0817c7658a3eb014cf1050b15fe",
        "_cell_guid": "583b42fd-0c9e-4ee0-9ece-95d145ccbb55",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Counter(pos_tags).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "411c04010ddef48b7db4021fa1ed87095baac709",
        "_cell_guid": "3d53c27e-7976-4042-9790-e933dceba3a4"
      },
      "cell_type": "markdown",
      "source": "Here I'm trying to extract some part of speech which can be useful for further steps."
    },
    {
      "metadata": {
        "_uuid": "e512986a789fecf9097ef6fd8c2016fb184e3596",
        "_cell_guid": "d8df26d7-6a3d-4d37-8cb0-ae1022486270",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def is_verb(token):\n    return token.pos == spacy.parts_of_speech.VERB",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b82f19f0838f42e8f02dc894a1b7bf0fa93ced20",
        "_cell_guid": "a5b82515-80d3-4eb7-9a22-767841c98625",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "verbs = set()\nfor t in eap:\n    if is_verb(t):\n        verbs.add(t.head)\nlen(verbs),Counter(verbs).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f64f8089120aaeaeb114c2994bf68db1e5a7525b",
        "_cell_guid": "b3e5d4b2-c367-48f7-a5c6-276aee516a18"
      },
      "cell_type": "markdown",
      "source": "Something wrong?\n(everything, 1)??? may you get different mistake.\n\n**Tip:** Good idea to extract right words, is to combine rules."
    },
    {
      "metadata": {
        "_uuid": "6eb94997e01809360bbb57c8635bbb8c84426c8e",
        "_cell_guid": "d4eec880-262a-4b37-b9c8-7fb3c3a1a06c",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "verbs = set()\nfor subject in eap:\n    if subject.dep == nsubj and subject.head.pos == VERB:\n        verbs.add(subject.head)\nlen(verbs),Counter(verbs).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0ac5ab33f5179630040e9b3b9f89e66054ba1e3d",
        "_cell_guid": "d2681888-d609-4810-96b7-67195e07a836",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def is_adverb(token):\n    return token.pos == spacy.parts_of_speech.ADV\nadverbs = set()\nfor t in eap:\n    if is_adverb(t):\n        adverbs.add(t.head)\nlen(adverbs),Counter(adverbs).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a0260672fe6a20a02b996007da36fbe519a791b4",
        "_cell_guid": "2343ad5a-61f2-45d0-a841-964e562930ad",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def is_adjective(token):\n    return token.pos == spacy.parts_of_speech.ADJ\nadjectives = set()\nfor t in eap:\n    if is_adjective(t):\n        adjectives.add(t.head)\nlen(adjectives),Counter(adjectives).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f5e7e20e556d7ed5d6171d73d94ab821223819e9",
        "_cell_guid": "a5b1d79b-e7cd-473f-805a-de96948260ff",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Entity recognition\nEntity recognition is the process of classifying named entities found in a text into pre-defined categories, such as persons, places, organizations, dates, etc."
    },
    {
      "metadata": {
        "_uuid": "13cd07d8b0f19b97585e89a929785def1eb927cd",
        "_cell_guid": "9119347f-d867-4ef8-99d3-514e44f61e2e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "entities = [(i,i.label_) for i in eap.ents] \nlen(entities),Counter(entities).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dfed7f59c819b96e30db7e0a1d5c0d5a69325f0b",
        "_cell_guid": "50918b23-8c55-4a00-8e49-59b43a64ad9b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#count just labels\nlabels = [i.label_ for i in eap.ents] \nlen(labels),Counter(labels).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81e216a3cbf2e03a30fa872f47736244cb0944e5",
        "_cell_guid": "2d373367-951e-451f-980b-ca2af59ce519",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#If you want to know a label meaning\nspacy.explain('GPE')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d327703bf807f5a803ef9a1dd293cddc2c02feef",
        "_cell_guid": "4be805a3-8fe7-401e-b201-0b420314512b"
      },
      "cell_type": "markdown",
      "source": "To learn more and master spacy's linguistic features see : https://spacy.io/usage/linguistic-features"
    },
    {
      "metadata": {
        "_uuid": "ca5831c8a6aba9f274de2c560191a02348f466b6",
        "_cell_guid": "3f826f48-2ddc-47a5-8fa6-51602a877b0b"
      },
      "cell_type": "markdown",
      "source": "**SERIOUS WORK:**\n\n1. Cleaning each document,(N.B: some sentences are for French language.)\n1. Extract significant words related to the author style (adjectives, verbs, adverbs)\n1. word embedding\n1. RNN model for training\n\n"
    },
    {
      "metadata": {
        "_uuid": "a544129515ec2454f3a04f78bf19a827baa0de36",
        "_cell_guid": "4b47e92d-4b2b-4afc-956f-5a5bd5d61e62"
      },
      "cell_type": "markdown",
      "source": "**STEP 1: CLEANING**"
    },
    {
      "metadata": {
        "_uuid": "e02b175332521d785c8b304d30dc77c6c62c9bfb",
        "_cell_guid": "84037f90-9cd5-495e-8ac6-27bba447fd91"
      },
      "cell_type": "markdown",
      "source": "During my experience with this text I've noticed:\n1. some mistake such as (\"Bransby's?Wrap\",),('ensue.i', 1) \n1. some similar names 'Elah Gabalah',  'Elah Gabalus' (I don' know if it's a mistake or they are different person).\n1. (\"dxn't dx\", 1),  ('massa de lef eye jis', 1),?????? not entities"
    },
    {
      "metadata": {
        "_uuid": "e680bb858867d8857ec81189911fbc3bbfeaac91",
        "_cell_guid": "779748c2-bde7-494a-af93-dad7e5db945e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Useful functions:\n# transform lists to texts\ndef list_to_text(l):\n    text = \" \"\n    for s in range(len(l)):\n        sent = \"\".join(l[s]).strip()\n        text += sent\n    return text\n# Clean up the texts\ndef cleanText(text):\n    text = text.strip().replace(\"?\",\"? \").replace(\".\",\". \")\n    text = text.lower()\n    return text\n# Get the documents with spacy\ndef to_nlp(text):\n    nlp = spacy.load('en')\n    cleaned_text = cleanText(text)\n    document = nlp(cleaned_text)\n    return document",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "32b0d49f18e52de2b8b006e87f596fd7cbd33bbf",
        "_cell_guid": "ebb3a9bb-7638-4c61-9fbd-e3fdb28e9f1e"
      },
      "cell_type": "markdown",
      "source": "**First author EAP:**"
    },
    {
      "metadata": {
        "_uuid": "ec18fc56fc021d34cc6ac344151043f6fa87bb22",
        "_cell_guid": "4d56b2c5-d35e-46c8-9200-4fc2e9481a03",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "list_EAP = list(train.text[train.author == \"EAP\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "80c07f446b3cfb1cf513194504be3f6d802ec975",
        "_cell_guid": "cc03c767-6593-4e9b-9192-b9120341a88f",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "text_eap = list_to_text(list_EAP)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b887823f70bda832c9cd4965b0291a0cbdcc57e0",
        "_cell_guid": "928e459f-1886-47d2-8480-f68218e2dfce",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "eap = to_nlp(text_eap)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94627b190afdb0aeabbbd7141fbdc1dfe7bbd6b4",
        "_cell_guid": "6cf58b66-0cb8-4e65-92de-4a6565804411",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "len(eap)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "117ceef34539b65dc747ba105e1fdb876a37b9c7",
        "_cell_guid": "72592851-edd0-4fa5-a086-75eababbbcd9",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "**Second author HPL:**"
    },
    {
      "metadata": {
        "_uuid": "74c3fd80ab005d4fc44fb1da59451e61ebd56832",
        "_cell_guid": "29a7e936-abea-4990-961d-fdaeee3a14f1",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "list_HPL = list(train.text[train.author == \"HPL\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b6d3026285fe52b1ebd979f4d5543c9039584a6d",
        "_cell_guid": "2affdaf8-69b9-4d5c-b5a0-4238b8ce3e32",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "text_hpl = list_to_text(list_HPL)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a151b3c00d61d114e6361918e4914d03d027ac4",
        "_cell_guid": "585e06fe-64b3-49ec-9955-735aeff9a822",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "hpl = to_nlp(text_hpl)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "957e5222781f6b55938b8ad1ca43e6e7696982be",
        "_cell_guid": "0c97f4f3-8599-44ec-ae75-1705396c8e7c",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(hpl)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd9b8a74285809f5dca89fc292240d104d1919ba",
        "_cell_guid": "50c06a70-caef-45f7-bdea-5d1b9c5b3acb"
      },
      "cell_type": "markdown",
      "source": "**Third author MWS:**"
    },
    {
      "metadata": {
        "_uuid": "8c68cdee15daca29a37479fdb691421639d9097d",
        "_cell_guid": "413da82f-d90c-4215-8167-bbf8b40d91d3",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "list_MWS = list(train.text[train.author == \"MWS\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "940dc8e510867f0c2fe871ff7cc19e7afde57c7b",
        "_cell_guid": "1692f62a-d299-4ec7-8d2d-72270def34ac",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "text_mws = list_to_text(list_MWS)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d937552781edc4a103be47ff02c8978ef8a56760",
        "_cell_guid": "3789a2eb-90c9-4876-b746-a8ac4421b3d9",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "mws = to_nlp(text_mws)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69303a0c4c5e9622ceccb8e4890dbbb0720d0d2f",
        "_cell_guid": "74f3bf40-d953-4c24-af91-6305453f0d31",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(mws)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0fc9a4524f6bb44cc2e506c1b06386d6ceb45d62",
        "_cell_guid": "86cfeda8-125f-4381-904e-db77cd258d9a"
      },
      "cell_type": "markdown",
      "source": "**STEP2: EXTRACTING WORDS**"
    },
    {
      "metadata": {
        "_uuid": "3ae0dfccb6d58a4d4b6a15f2ffc4d8778dd57926",
        "_cell_guid": "1a4856ee-7b77-4b33-a4f7-aae55ba592d5",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from collections import defaultdict\ndef dict_words(list_words,limit):\n    counter = defaultdict(int)\n    for k in list_words:\n        counter[k] += 1\n    dict_words = sorted(counter.items(),key=lambda x: x[1])\n    highly_used_words=list()\n    for i in range(len(dict_words)):\n        if dict_words[i][1]>=limit:\n            highly_used_words.append([dict_words[i][0]])\n    return highly_used_words",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "63d9543b31fef25039cbe4a2c47fa050bd8cb9ce",
        "_cell_guid": "8122bfcb-35b0-4fb8-96da-979908cd833e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "verbs = list()\nfor subject in eap:\n    if subject.dep == nsubj and subject.head.pos == VERB:\n        verbs.append(subject.head.lemma_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e089467fe3050f9a136430f6d3cc462bc83cf3e8",
        "_cell_guid": "5043dbc7-b18e-41a7-81d5-2e6d94ced0c8",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#highly_used_verbs\nhighly_used_verbs = dict_words(verbs,100)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a8e76babc6eaed6a2ee268080f36893486251f3c",
        "_cell_guid": "6cf4dac8-5fa6-45da-b686-bf46d37896fa",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#just testing here....\neap_heros = list()\nfor ent in eap.ents:\n    if ent.label_ in ['PERSON','NORP','GPE']:\n        eap_heros.append(ent)\nlen(eap_heros),Counter(eap_heros).most_common(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "30bacb6c301def7aabf9a50b93b8ff4a4f88639a",
        "_cell_guid": "2cee9f6c-aa1f-42d5-86ae-a9059896362a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "highly_used_heros = dict_words(eap_heros,1)\n#highly_used_heros",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0be8b7599a6e9c89ebf7495c6153c1b1d7e0f49d",
        "_cell_guid": "6375af87-8322-421a-a04c-be4a9386de7a"
      },
      "cell_type": "markdown",
      "source": "[\"],[j],[ ],[dxn't dx], [however], [massa de lef eye jis] are mistakes."
    },
    {
      "metadata": {
        "_uuid": "f65b86caad2ce26d82b743005135055dd8fe7822",
        "_cell_guid": "afdfee0e-2584-403f-be8b-d26bbbf7438d",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# For each author I'll select unique vocabulary\n#eap_vocabulary = highly_used_verbs + highly_used_heros + ....",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "af80a29cf914bd3a5c23447f9cf70f593e41fa1b",
        "_cell_guid": "735a31a8-c6ac-4e8e-8f6d-02ce958b3c5d"
      },
      "cell_type": "markdown",
      "source": "**STEP3: WORD EMBEDDING**"
    },
    {
      "metadata": {
        "_uuid": "bd54151c47b8a73639cb2406ac00a16ab0933629",
        "_cell_guid": "080943bf-cfa9-41aa-a075-659e023c7c3d"
      },
      "cell_type": "markdown",
      "source": "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n\n**What are Word Embeddings?**\n\nIn very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. But before we dive into the details of Word Embeddings, the following question should be asked – Why do we need Word Embeddings?\n\n**Different types of Word Embeddings:**\n\nThe different types of word embeddings can be broadly classified into two categories:\n*     Frequency based Embedding\n*     Prediction based Embedding"
    },
    {
      "metadata": {
        "_uuid": "4e2ac63360f5192f0b2a1a226bbb9907cc5746b2",
        "_cell_guid": "02a2632b-e737-4ec2-8568-1a3008a9fe78",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from gensim.models import Word2Vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9dd4377ea53db0c3da3793f49eebb81c9003ac38",
        "_cell_guid": "94ff561d-d2c6-4acd-910d-b5d0320d8103"
      },
      "cell_type": "markdown",
      "source": "https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n\n* **size**: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n* **window**: (default 5) The maximum distance between a target word and words around the target word.\n* **min_count**: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n* **workers**: (default 3) The number of threads to use while training.\n* **sg**: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
    },
    {
      "metadata": {
        "_uuid": "0ab720d4b36601f530a41a35c10297a20f567f8d",
        "_cell_guid": "a21a3d1b-8f74-41b3-8429-5e95bc131d59",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# train model\nmodel = Word2Vec(highly_used_verbs, min_count=1)\n# summarize the loaded model\nprint(model)\n# summarize vocabulary\nwords = list(model.wv.vocab)\nprint(words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca8bc9b5bf27caf141ce5c16e90ba70a87b72b7d",
        "_cell_guid": "ce9b13b3-3161-4d3e-80bf-65e5fde4ef29",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "vocabulary_size = len(model.wv.vocab) #vocab=19\nvocabulary_size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f24c47284520d78818db07e32d828da54fba8e5a",
        "_cell_guid": "e339481c-6db9-4388-90fc-91b58772f54c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# access vector for one word size = 100\nprint(model['be'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c13515b10b7ea5ac41612318bb0bc0c9bd7e577",
        "_cell_guid": "c7efef25-b569-4bc8-9fa0-40d7bd8375eb",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model.wv.index2word[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee370b3d98a8e38dc35b0f584dcf513e60a3026f",
        "_cell_guid": "a454ea6d-3442-4280-820c-d6fe2d3dacf1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\nfrom matplotlib import pyplot\n# fit a 2d PCA model to the vectors\nX = model[model.wv.vocab]\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(model.wv.vocab)\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "daad09ddaa99fa53e34cc212a9edf9ad15c01728",
        "_cell_guid": "5601300e-5e87-4cf3-ab8f-8166e01fbf2b",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "**STEP4: RNN**"
    },
    {
      "metadata": {
        "_uuid": "7a34f2ef29da91657952c1144fe0aeb76a208090",
        "_cell_guid": "ac164a43-aa51-4e99-a0e4-c4a611397b1c"
      },
      "cell_type": "markdown",
      "source": "* http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/\n* https://www.tensorflow.org/tutorials/word2vec\n* https://spacy.io/usage/training#section-tips\n* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n* https://github.com/crazydonkey200/tensorflow-char-rnn"
    },
    {
      "metadata": {
        "_uuid": "9c5d7d104f9ec40850b16127d327afc68c6d7138",
        "_cell_guid": "615c6372-ca9f-423e-9305-6cf30f0f0141",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# For insertion into TensorFlow let's convert the wv word vectors into a numpy matrix\nvocabulary_size = len(model.wv.vocab)\nvector_dim = 100 #model size\nembedding_matrix = np.zeros((vocabulary_size, vector_dim))\nfor i in range(vocabulary_size):\n    embedding_vector = model.wv[model.wv.index2word[i]]\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1dbe4cb85b8e4807933fb3ad1ea6925112013b6a",
        "_cell_guid": "25456da3-f830-45d8-94c2-cadb4f140542",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#embedding_matrix[0][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "454bdb12ff866b12821da89073ab51f09d977d8e",
        "_cell_guid": "0aa112be-4a6b-4afa-8dcd-44197e459f19"
      },
      "cell_type": "markdown",
      "source": "To refresh memory or to be familiar with tensorflow see : http://adventuresinmachinelearning.com/python-tensorflow-tutorial/ "
    },
    {
      "metadata": {
        "_uuid": "8a05056718ff2fab59f2e8a08ecc7a83028e2035",
        "_cell_guid": "749faa3a-e297-4a08-8437-cdcdb81a43dc",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import tensorflow as tf \n#Using gensim Word2Vec embeddings in TensorFlow\n# embedding layer weights are frozen to avoid updating embeddings while training\nfrozen_embeddings = tf.constant(embedding_matrix)\nembedding = tf.Variable(initial_value=frozen_embeddings, trainable=False)\n# trainable=False, otherwise the embedding layer would be trained with negative performance impacts. ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e7347f19b0edb00edcfcbd465e5c6d0731851ab4",
        "_cell_guid": "8d9a775c-fa98-4017-a953-7594ccec11fa"
      },
      "cell_type": "markdown",
      "source": "**STARTING RNN MODEL**:\n* http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n* http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n* http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"
    },
    {
      "metadata": {
        "_uuid": "343129b2fae21ca47abbcd5689455333d8a56a69",
        "_cell_guid": "cd2b53b5-4fa8-4b5f-88ec-c34f013a9e1a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "name": "python",
      "mimetype": "text/x-python",
      "version": "3.6.3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}